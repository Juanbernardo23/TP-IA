## Implementaci√≥n de Modelos de Lenguaje de Gran Tama√±o (LLM) en la resoluci√≥n de problemas matem√°ticos

Primer Fecha de presentaci√≥n
24/5/2025

_Juan Antonio Bernardo_


# Qu√© es un LLM

Un modelo de lenguaje de gran tama√±o (LLM, ‚ÄúLarge Language Model‚Äù) es un tipo de red neuronal profunda entrenada sobre enormes cantidades de texto para ‚Äúaprender‚Äù patrones estad√≠sticos del lenguaje natural. A grandes rasgos, estos modelos se fundamentan en la arquitectura Transformer. La particularidad de los LLM consiste en su tama√±o: suelen tener miles de millones (o incluso centenas de miles de millones) de par√°metros entrenables, lo que les permite capturar relaciones complejas entre palabras, oraciones y documentos completos.
Aplicaciones de LLM en educaci√≥n y en resoluci√≥n de problemas matem√°ticos
Los LLM han abierto nuevas posibilidades en el √°mbito educativo, tanto para docentes como para estudiantes. A continuaci√≥n se describen algunas de las aplicaciones m√°s relevantes:
Asistentes de estudio personalizados: Un LLM puede actuar como tutor virtual, contestando preguntas de manera inmediata y adaptando la explicaci√≥n seg√∫n el nivel de comprensi√≥n del alumno.
Generaci√≥n de ejercicios y retroalimentaci√≥n autom√°tica: Los LLM pueden generar conjuntos de problemas de pr√°ctica con variaciones en la dificultad y el formato de los enunciados. Asimismo, ante un enunciado resuelto por el alumno, el LLM puede revisar la soluci√≥n y se√±alar errores en el razonamiento simb√≥lico o aritm√©tico, indicando en qu√© paso se produjo la falla.

Situaci√≥n particular e inicial

## Hardware disponible:

__IMAGEN 1__

Como an√°lisis inicial, mi hardware personal es decente para un entrenamiento ligero y ajustes m√≠nimos de un LLM, especialmente sin una GPU dedicada. Aun as√≠, luego de investigar e introducirnos en tema con t√©cnicas como LoRA + CPU-friendly training y buena planificaci√≥n, se puede lograr un entrenamiento funcional, aunque m√°s lento y limitado.



# LoRA

LoRA (Low-Rank Adaptation) es una t√©cnica moderna de fine-tuning para modelos grandes como LLMs (Large Language Models), que permite ajustar modelos enormes usando pocos recursos de hardware, como una PC sin GPU potente.

## ¬øQu√© hace LoRA?
En lugar de ajustar todos los millones o billones de par√°metros del modelo original (lo cual es muy costoso en memoria y tiempo), LoRA:

Congela el modelo original.
Agrega capas peque√±as y livianas (matrices de bajo rango) dentro del modelo.
Solo entrena esas capas nuevas, manteniendo intacto el modelo base.

Requisitos para usar LoRA

Se necesitan las siguientes librer√≠as en nuestra PC, como m√≠nimo, al momento de hacer fine tuning:

transformers
peft
accelerate
datasets
bitsandbytes


Recomendaciones adicionales

Adem√°s, se nos recomend√≥ inicialmente usar un tama√±o de modelo peque√±o, usar batch size = 1 y grad_accum para reducir el uso de memoria RAM dram√°ticamente al entrenar, a su vez, entrenar por tramos y guardar checkpoints y evitar tokenizaciones complejas.

Mi GPU integrada Radeon no soporta entrenamiento con frameworks como PyTorch + CUDA, ni siquiera a trav√©s de ROCm (soporte a√∫n limitado).

Primeros pasos

Como primera decisi√≥n, eleg√≠ e instal√© LLM Studio porque el programa me pareci√≥ bastante amigable para empezar a experimentar con un LM. 

Mi primera opci√≥n en cuanto a modelo, seg√∫n la recomendaci√≥n del programa, fue Qwen2.5-Math-7B, IA que seg√∫n internet es de lenguaje de 7 mil millones de par√°metros optimizado para razonamiento matem√°tico. Forma parte de la serie Qwen2.5 desarrollada por Alibaba y destaca por su capacidad para resolver problemas matem√°ticos con m√∫ltiples pasos, incluyendo √°lgebra, aritm√©tica y l√≥gica simb√≥lica. Es compatible con formatos como LaTeX y funciona bien en tareas que requieren precisi√≥n y seguimiento detallado de instrucciones matem√°ticas.

El modelo fue probado desde el mismo LLM Studio, haciendo una consulta breve en espa√±ol, ‚Äú¬øcu√°nto es la ra√≠z cuadrada de 81?‚Äù la cual gener√≥ una algo tard√≠a respuesta, usando bastante RAM y obtuve una respuesta muy larga aunque acertada, pero completamente en ingl√©s. 

__IMAGEN 2__

A partir de eso, se tom√≥ la decisi√≥n de seguir viendo otras opciones para luego tratar de hacer un fine tuning en espa√±ol orientado a lo visto en la materia, adem√°s de que buscando en internet, pude llegar a la conclusi√≥n de que la Qwen2.5-MAth-7B era un modelo bastante pesado para mi PC, (16 GB de Ram, un procesador bastante bueno pero una GPU integrada de 496 MB nom√°s) as√≠ que ese iba a ser el gran limitante, adem√°s del idioma ya que la idea es hacer prompts en espa√±ol a largo plazo. Me instru√≠ sobre c√≥mo era la estructura y la forma de instalar otra IA localmente para su posterior intento de fine tuning, preferente usando VSC, en lo que pude aprender a cada escala que:

üìÑ train.py
Script principal que realiza el fine-tuning del modelo.
Carga los datos de entrenamiento, define la funci√≥n de p√©rdida y guarda el modelo ajustado localmente.

üìÑ data/train.json
Archivo de entrenamiento en formato JSON, con una entrada por l√≠nea.
La idea en √©ste caso, fue grupalmente generar un JSON con varios ejercicios matem√°ticos, y para la segunda instancia de la investigaci√≥n lograr un entrenamiento mucho m√°s avanzado.
El JSON cuenta con 350 l√≠neas. El contenido incluye ejercicios matem√°ticos de dificultad progresiva, desde temas introductorios hasta problemas m√°s complejos. Cada entrada est√° estructurada como un intercambio tipo chat, con un mensaje del usuario y una respuesta del asistente. Hacia el final del conjunto se encuentran ejemplos avanzados, como desigualdades con valor absoluto y demostraciones algebraicas, lo que permite al modelo captar tanto el razonamiento simb√≥lico como la claridad explicativa esperada.



Ej fragmento del JSON utilizado:

__IMAGEN 3__

üìÑ config.json
Archivo opcional para almacenar par√°metros como nombre del modelo, n√∫mero de √©pocas o rutas de archivos.
Permite centralizar la configuraci√≥n del entrenamiento para facilitar ajustes futuros.

üìÑ chat.py
Script para interactuar con el modelo fine-tuneado en forma de chat local.
Recibe una entrada del usuario y devuelve la respuesta m√°s sem√°nticamente cercana entre una lista predefinida.

## paraphrase multilingual minilm l12 v2

Luego de seguir buscando otros modelos de IA convenientes para mi PC, idioma y al menos ahora un ‚Äúentrenamiento simple‚Äù, me convenc√≠ de probar paraphrase multilingual minilm l12 v2 dise√±ada seg√∫n las fuentes para generar representaciones vectoriales densas de 384 dimensiones a partir de oraciones o p√°rrafos en m√°s de 50 idiomas. Estas representaciones son √∫tiles para tareas como b√∫squeda sem√°ntica, agrupamiento de textos y comparaci√≥n de similitud entre frases. En √©ste caso, con algo ya m√°s de idea sobre c√≥mo deb√≠a proceder y observando que √©ste modelo pesaba alrededor de 1,5 GB, trat√© de directamente armar la estructura ‚Äúmodelo‚Äù para tratar de entrenarla localmente, ayud√°ndome con ChatGPT para algunos ajustes, y que la misma vaya tomando forma, pude terminar el entrenamiento luego de muchos intentos durante varios d√≠as donde me fue dando muchos errores relacionados al CPU, al modelo en s√≠ y a los comandos t√©cnicos en el c√≥digo. Aqu√≠ algunas capturas de pantalla de lo que fue el proceso, probando con varios ajustes en el train.py de distintos comandos, como:


__IMAGEN 4__


Captura del entrenamiento realiz√°ndose despu√©s de varios errores en el proceso:

__IMAGEN 5__

El entrenamiento finalmente tard√≥ alrededor de dos horas y media una vez que pude dar con las configuraciones aparentemente correctas, tomando bien el JSON, y luego de √©sto lleg√≥ el momento de probar el chat fine-tuneado mediante el script del chat.py, donde termin√© viendo la primera salida: 

__IMAGEN 6__

√âsto fue muy interesante, porque adem√°s de detectar que la codificaci√≥n estaba siendo muy extra√±a, la respuesta tard√≥ alrededor de 5 minutos, utilizando toda la CPU y RAM. 

__IMAGEN 7__

Finalmente, despu√©s de muchos intentos fallidos y de buscar un poco m√°s de informaci√≥n sobre el modelo, encontr√© que en realidad:
‚ÄúNo est√° entrenado para matem√°ticas: su arquitectura no entiende s√≠mbolos matem√°ticos ni realiza razonamiento formal como lo har√≠an otros modelos.

## Phi-2

Por lo que se tuvo que tomar la decisi√≥n de dejar de perder tiempo y buscar otro candidato, el cual termin√≥ siendo Phi-2, de Microsoft, un modelo de 2.7 mil millones de par√°metros (2.7B), y que viene con un fuerte enfoque en calidad y eficiencia, ideal para tareas de lenguaje general, razonamiento y c√≥digo ligero. Tambi√©n seg√∫n investigu√©, es ideal para aplicar fine-tuning.

La estructura de las carpetas y archivos, luego de descargar localmente la IA y proponer el entrenamiento, finalmente fue:

mi-proyecto-finetuning/

‚îú‚îÄ‚îÄ data/

‚îÇ   ‚îî‚îÄ‚îÄ train.json                     # Dataset de entrenamiento con conceptos matem√°ticos

‚îú‚îÄ‚îÄ phi-2/                                # Modelo original sin fine-tuning

‚îÇ   ‚îî‚îÄ‚îÄ (...)                              # Archivos del modelo base Phi-2

‚îú‚îÄ‚îÄ phi-2-finetuned/                # Modelo ya ajustado

‚îÇ   ‚îú‚îÄ‚îÄ added_tokens.json

‚îÇ   ‚îú‚îÄ‚îÄ config.json

‚îÇ   ‚îú‚îÄ‚îÄ generation_config.json

‚îÇ   ‚îú‚îÄ‚îÄ merges.txt

‚îÇ   ‚îú‚îÄ‚îÄ model.safetensors.index

‚îÇ   ‚îú‚îÄ‚îÄ model-00001-of-00003.safetensors

‚îÇ   ‚îú‚îÄ‚îÄ model-00002-of-00003.safetensors

‚îÇ   ‚îú‚îÄ‚îÄ model-00003-of-00003.safetensors

‚îÇ   ‚îú‚îÄ‚îÄ special_tokens_map.json

‚îÇ   ‚îú‚îÄ‚îÄ tokenizer/

‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_config.json

‚îÇ   ‚îú‚îÄ‚îÄ training_args.bin

‚îÇ   ‚îî‚îÄ‚îÄ vocab.json

‚îú‚îÄ‚îÄ checkpoints/                    # Checkpoints intermedios del fine-tuning

‚îÇ   ‚îî‚îÄ‚îÄ checkpoint-*/               # Subcarpetas con estados intermedios

‚îú‚îÄ‚îÄ config.py                       # Configuraci√≥n general del proyecto

‚îú‚îÄ‚îÄ datamodule.py                  # L√≥gica de carga y procesamiento del dataset

‚îú‚îÄ‚îÄ train.py                        # Script principal de entrenamiento

‚îú‚îÄ‚îÄ chat.py                         # Script para cargar el modelo finetuneado y responder preguntas



El proceso de los entrenamientos fueron largos, pasando por muchos intentos con fallas las cuales tardaban muchas horas, luego al probarlas daban errores de codificaci√≥n, ya que luego de analizar con detenimiento y generar alertas v√≠a c√≥digo, identificaba que el entrenamiento creaba muchos tokens vac√≠os en el tokenizer.json de salida (archivos muy largos), pero a eso solo lo pod√≠a comprobar entrenando el modelo y esperando, adem√°s de intentar adaptar el script de chat, quien tambi√©n podr√≠a estar dando un mal resultado:

__IMAGEN 8__

Se estuvieron varios d√≠as para la b√∫squeda de entrenamiento correcto, hasta que por fin luego de adaptar algunos par√°metros, se pudo crear una buena tokenizaci√≥n. El entrenamiento, a diferencia de los anteriores, tard√≥ una hora m√°s (aproximadamente 3 hs y media, los anteriores fallidos tardaban 2 hs y media, que no era poco) utilizando toda la RAM y el CPU en todos los casos:

__IMAGEN 9__

Una vez que se identific√≥ v√≠a chat.py que el entrenamiento fue correcto y que s√≥lo faltaba adaptar el script del chat, comenc√© a ver resultados, como un prompt ingresado muy b√°sico para probar la respuesta, y una demora de 2 minutos sumado a una gran utilizaci√≥n de RAM para finalmente dar con una alucinaci√≥n inicial:

__IMAGEN 10__

Luego, en algunos otros casos se busc√≥ una comprobaci√≥n, dando una respuesta con otros n√∫meros y mezclando una palabra en ingl√©s con otra en espa√±ol

__IMAGEN 11__

Luego de unos ajustes, se hicieron pruebas quiz√°s un poco m√°s complejas, dando una respuesta muy larga y un poco il√≥gica en un caso:

__IMAGEN 12__

Pero despu√©s de modificaciones, comenz√≥ a dar respuestas correctas, aunque en un primer momento en algunas oportunidades segu√≠a respondiendo con palabras en ingl√©s, aunque con resultados correctos. Adem√°s de que pude verificar que el c√≥digo ascii de la ra√≠z cuadrada, por ejemplo, lo tomaba bien.

__IMAGEN 13__

Finalmente, se pudieron corregir las estrategias de respuesta, tokenizaci√≥n, condiciones para tomar el prompt, etc, y las respuestas fueron l√≥gicas en la gran mayor√≠a de los casos:

__IMAGEN 14__

__IMAGEN 15__

En los prompts m√°s complejos, el tiempo de respuesta ha llegado hasta los 4 minutos, y en varios casos, a pesar de que en el JSON ten√≠amos bastante entrenamiento preciso para casos complejos, no termin√≥ de afianzarse l√≥gica en algunas respuestas:

__IMAGEN 16__

